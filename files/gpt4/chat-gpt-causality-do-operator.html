<!DOCTYPE html><html><head>  <meta charset="utf-8"/>  <title>Chat GPT: Causality: Do Operator</title>  <meta name="generator" content="chatGPT Saving Bookmark"/><style>body {  background-color: rgb(32,33,35);  color: rgb(236,236,241);  font-size: 16px;  font-family: sans-serif;  line-height: 28px;  margin: 0;}body > .w-full {  padding: 30px;}/* prompt */body > .w-full:nth-child(2n+1) {  background: rgb(52,53,65);}/* response */body > .w-full:nth-child(2n+2) {  background: rgb(68,70,84);}a, a:visited {  color: #7792cd;}pre {  margin: 0 0 1em 0;  display: inline-block;  width: 100%;}pre code.hljs {  margin-bottom: 1em;  border-radius: 5px;}.whitespace-pre-wrap {  white-space: pre-wrap;}.flex-col {  max-width: 850px;  margin: 0px auto;}*, :after, :before {    border: 0 solid #d9d9e3;    box-sizing: border-box;}table {    border-collapse: collapse;    border-color: inherit;    text-indent: 0;}.markdown table {    --tw-border-spacing-x: 0px;    --tw-border-spacing-y: 0px;    border-collapse: separate;    border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y);    width: 100%}.markdown th {    background-color: rgba(236,236,241,.2);    border-bottom-width: 1px;    border-left-width: 1px;    border-top-width: 1px;    padding: .25rem .75rem}.markdown th:first-child {    border-top-left-radius: .375rem}.markdown th:last-child {    border-right-width: 1px;    border-top-right-radius: .375rem}.markdown td {    border-bottom-width: 1px;    border-left-width: 1px;    padding: .25rem .75rem}.markdown td:last-child {    border-right-width: 1px}.markdown tbody tr:last-child td:first-child {    border-bottom-left-radius: .375rem}.markdown tbody tr:last-child td:last-child {    border-bottom-right-radius: .375rem}/* chatGPT code color theme */code.hljs,code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#fff;-webkit-hyphens:none;hyphens:none;line-height:1.5;tab-size:4;text-align:left;white-space:pre;word-break:normal;word-spacing:normal}pre[class*=language-]{border-radius:.3em;overflow:auto}:not(pre)>code.hljs,:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.hljs-comment{color:hsla(0,0%,100%,.5)}.hljs-meta{color:hsla(0,0%,100%,.6)}.hljs-built_in,.hljs-class .hljs-title{color:#e9950c}.hljs-doctag,.hljs-formula,.hljs-keyword,.hljs-literal{color:#2e95d3}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#00a67d}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#df3079}.hljs-bullet,.hljs-link,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#f22c3d}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#a9aec1}.token.punctuation{color:#fefefe}.token.constant,.token.deleted,.token.property,.token.symbol,.token.tag{color:#ffa07a}.token.boolean,.token.number{color:#00e0e0}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#abe338}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url,.token.variable{color:#00e0e0}.token.atrule,.token.attr-value,.token.function{color:gold}.token.keyword{color:#00e0e0}.token.important,.token.regex{color:gold}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}@media screen and (-ms-high-contrast:active){code[class*=language-],pre[class*=language-]{background:window;color:windowText}:not(pre)>code[class*=language-],pre[class*=language-]{background:window}.token.important{background:highlight;color:window;font-weight:400}.token.atrule,.token.attr-value,.token.function,.token.keyword,.token.operator,.token.selector{font-weight:700}.token.attr-value,.token.comment,.token.doctype,.token.function,.token.keyword,.token.operator,.token.property,.token.string{color:highlight}.token.attr-value,.token.url{font-weight:400}}/* style of the code snippets */.rounded-md {    border-radius: 0.375rem;}.mb-4 {    margin-bottom: 1rem;}.p-4 {    padding: 1rem;}.py-2 {    padding-bottom: 0.5rem;    padding-top: 0.5rem;}.px-4 {    padding-left: 1rem;    padding-right: 1rem;}.text-xs {    font-size: .75rem;    line-height: 1rem;}.bg-black {    --tw-bg-opacity: 1;    background-color: rgba(0,0,0,var(--tw-bg-opacity));}.text-gray-200 {    --tw-text-opacity: 1;    color: rgba(217,217,227,var(--tw-text-opacity));}.bg-gray-800 {    --tw-bg-opacity: 1;    background-color: rgba(52,53,65,var(--tw-bg-opacity));}.rounded-t-md {    border-top-left-radius: 0.375rem;    border-top-right-radius: 0.375rem;}code.hljs, code[class*=language-], pre[class*=language-] {    word-wrap: normal;    background: none;    color: #fff;    -webkit-hyphens: none;    hyphens: none;    line-height: 1.5;    tab-size: 4;    text-align: left;    white-space: pre;    word-break: normal;    word-spacing: normal;}.prose :where(code):not(:where([class~=not-prose] *)) {    color: var(--tw-prose-code);    font-size: .875em;    font-weight: 600;}.prose :where(pre):not(:where([class~=not-prose] *)) {    background-color: transparent;    border-radius: 0.375rem;    color: currentColor;    font-size: .875em;    font-weight: 400;    line-height: 1.7142857;    margin: 0;    overflow-x: auto;    padding: 0;}.prose :where(pre code):not(:where([class~=not-prose] *)) {    background-color: transparent;    border-radius: 0;    border-width: 0;    color: inherit;    font-family: inherit;    font-size: inherit;    font-weight: inherit;    line-height: inherit;    padding: 0;}.\!whitespace-pre {    white-space: pre!important;}.overflow-y-auto {    overflow-y: auto;}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"/></head><body><div class="flex w-full items-center justify-center gap-1 border-b border-black/10 bg-gray-50 p-3 text-gray-500 dark:border-gray-900/50 dark:bg-gray-700 dark:text-gray-300">Model: Default (GPT-3.5)</div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">What's the do operator in causality?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>In causality, the do operator (also known as the intervention operator) is a notation used to indicate that a variable or set of variables has been manipulated or set to a specific value, in order to study the causal effect of that variable(s) on other variables in a system.</p><p>For example, suppose we are interested in the causal effect of a person's education level on their income, and we have data on both variables as well as other potential confounding variables (e.g., age, gender, etc.). If we simply observe the data, there may be many factors that influence income, and it may be difficult to disentangle the effect of education from other factors. However, by applying the do operator and setting the education level to a particular value (e.g., high school degree), we can isolate the causal effect of education on income while holding other factors constant.</p><p>The do operator is typically denoted using a symbol like "do(X = x)", where X represents the variable(s) being manipulated and x represents the value(s) they are set to. This notation allows us to express causal relationships in a precise and transparent way, and it is a key tool for causal inference in many fields, including epidemiology, economics, and machine learning.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">If we set do(education level = high school), how does this allow us to analyze the causal effect of education level on income? If we are only considering the high school level, then it's not a variable and we cannot even correlate it, let alone find a causal relation.</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>When we use the do operator to set education level to "high school," we are not trying to analyze the relationship between education level and income only at the high school level. Rather, we are manipulating the education level variable to create a hypothetical scenario in which everyone in the population has a high school education. In other words, we are assuming that education level is a causal variable that can be changed by an external intervention, and we are examining the causal effect of this intervention on income.</p><p>By setting education level to "high school" using the do operator, we can compare the income of individuals who have a high school education to the income of individuals who have a different education level, while holding other variables (such as age, gender, and occupation) constant. This allows us to estimate the causal effect of education level on income, which is the change in income that would result from a change in education level, while holding other variables constant.</p><p>Of course, in practice, we cannot actually intervene to set everyone's education level to high school. However, the do operator allows us to reason about the potential outcomes that would occur under different interventions, and to estimate the causal effects of those interventions using observational data. This is a key tool for causal inference in many fields, and it can help us to identify the factors that drive important outcomes such as income, health, and education.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">A lot of this seems like what a controlled clinical study attempts to do, by estimating the mean response for various different groups (factors).</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>You are correct that controlled clinical studies and experiments share some similarities with the use of the do operator in causal inference. Both approaches aim to estimate causal effects by manipulating one or more variables of interest while holding other variables constant.</p><p>In a controlled clinical study, researchers typically randomly assign participants to different treatment groups (or control groups) and measure the outcomes of interest for each group. By comparing the average outcomes across groups, researchers can estimate the causal effect of the treatment on the outcome, while holding other factors constant.</p><p>Similarly, in causal inference using the do operator, we manipulate one or more variables of interest (such as education level) while holding other variables constant, and we compare the outcomes of interest for different values of the manipulated variable. This allows us to estimate the causal effect of the variable on the outcome of interest, while accounting for the potential confounding effects of other variables.</p><p>However, there are also important differences between the two approaches. Controlled clinical studies typically involve randomization, which helps to ensure that the treatment groups are similar on average with respect to both observed and unobserved confounding factors. In contrast, causal inference using the do operator relies on assumptions about the structure of the causal model and the absence of unmeasured confounding factors. Additionally, the do operator can be used to estimate the causal effects of interventions that cannot be studied using randomized experiments, such as the effects of policies or natural disasters.</p><p>In summary, while controlled clinical studies and causal inference using the do operator share some similarities, they also have important differences and are used in different contexts. Both approaches are important tools for understanding causal relationships in complex systems.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">So, the do operator depends on a lot of assumptions? Even, say, clinical ANOVA studies have some assumptions, but normally those assumptions are empirically justifiable and based on the law of large numbers.</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Yes, the do operator does rely on a set of assumptions, as is the case with many statistical and causal inference methods. The assumptions underlying the do operator are typically based on the structural relationships between variables in the causal system being studied, and they are used to infer the causal effects of manipulating one or more variables on other variables in the system.</p><p>One key assumption in using the do operator is that the causal structure of the system is known and can be represented by a causal graph or model. This assumption requires a deep understanding of the underlying mechanisms that drive the system, and it may be difficult to validate in practice. Additionally, the assumptions underlying the do operator may be violated in certain circumstances, such as when there are unmeasured confounding variables that influence both the treatment and the outcome, or when there are feedback loops or other complex dynamics in the system.</p><p>However, the assumptions underlying the do operator are often more transparent and easier to justify than the assumptions underlying other causal inference methods. For example, in linear regression or ANOVA, the assumption of linearity and normality may not always be empirically justifiable, but they are often used as a simplifying assumption that can be checked using diagnostic tools. Similarly, the assumptions underlying the do operator can be tested and validated using sensitivity analysis, which involves examining how the results of the analysis change when the assumptions are relaxed or modified.</p><p>In summary, while the do operator does rely on a set of assumptions, these assumptions are often more transparent and easier to justify than the assumptions underlying other causal inference methods. It is important to carefully consider these assumptions when using the do operator or any other causal inference method, and to conduct sensitivity analyses to assess the robustness of the results to potential violations of these assumptions.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Well, a structural assumption seems pretty big.</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Yes, the structural assumptions underlying the do operator are indeed important and can have a significant impact on the validity of the causal inference. The structural assumptions relate to the way in which the variables in the system are causally connected, and they are typically represented using a causal graph or model.</p><p>The assumptions underlying the causal graph or model include the directionality and strength of the causal relationships between variables, the presence or absence of confounding or mediating variables, and the structure of any feedback loops or other complex dynamics in the system. In many cases, the assumptions underlying the causal graph or model may not be directly observable, and they may need to be inferred based on domain knowledge, expert opinions, or empirical data.</p><p>To mitigate the potential impact of structural assumptions on the validity of the causal inference, sensitivity analysis can be used to examine the robustness of the results to different assumptions or variations in the causal graph or model. Sensitivity analysis involves varying the assumptions underlying the causal graph or model and examining how the results of the causal inference change in response to these variations. By conducting sensitivity analysis, researchers can identify the assumptions that are most critical to the validity of the causal inference and assess the degree of uncertainty or variability in the estimated causal effects.</p><p>In summary, while the structural assumptions underlying the do operator are important and can have a significant impact on the validity of the causal inference, sensitivity analysis can be used to assess the robustness of the results to different assumptions and variations in the causal graph or model. Careful consideration of the underlying assumptions and the use of sensitivity analysis can help to ensure that the causal inference is as reliable and valid as possible given the available data and domain knowledge.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">so, say I have three variables, R and C. I I have this graph, C &lt;- R. That is, A causes C, C does not cause R. (roughly)   What do operations can I apply to change it?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>In the causal graph where C &lt;- R, the arrow pointing from R to C indicates a causal relationship in which R is the cause and C is the effect. This means that changes in R will lead to changes in C, but changes in C will not affect R. To change the causal relationship between R and C, we can apply the do operator to manipulate the value of one or both variables.</p><p>Here are some examples of do operations that we can apply to the causal graph C &lt;- R to change the causal relationship between R and C:</p><ol><li><p>do(R = r): This operation sets the value of R to a specific value r, while leaving C unchanged. By applying this operation, we can observe the effect of changing R on C while holding all other variables constant. For example, we might set R to a high value and observe the corresponding change in C.</p></li><li><p>do(C = c): This operation sets the value of C to a specific value c, while leaving R unchanged. By applying this operation, we can observe the effect of changing C on other variables while holding R constant. For example, we might set C to a high value and observe the corresponding change in other variables that are influenced by C.</p></li><li><p>do(R = r, C = c): This operation sets the values of both R and C to specific values r and c, respectively. By applying this operation, we can observe the joint effect of changing both R and C on other variables in the system. For example, we might set R to a high value and set C to a low value, and observe the corresponding changes in other variables that are influenced by R and C.</p></li></ol><p>It is important to note that the specific effect of each do operation will depend on the underlying causal structure of the system and the presence or absence of other confounding variables. In some cases, applying the do operator may lead to biased or misleading results if the underlying assumptions are violated or if there are unobserved confounding variables. Therefore, it is important to carefully consider the underlying causal structure and assumptions, and to conduct sensitivity analysis to assess the robustness of the results to different variations of the causal model.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">How do we delete an edge using the do operator?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>The do operator is used to manipulate the values of variables in a causal model, and it is not directly used to delete or modify the edges in the causal graph. Instead, modifying the causal graph requires directly changing the structure of the graph itself.</p><p>In a causal graph, deleting an edge represents the intervention of setting the causal relationship between two variables to zero or removing the direct influence of one variable on another. This can be achieved by modifying the structure of the causal graph, either by physically removing the edge from the graph or by introducing a new variable that blocks the causal path between the two variables.</p><p>If we want to delete the edge between variables R and C in the causal graph C &lt;- R, we can achieve this by modifying the structure of the graph to reflect the new causal relationship between R and C. For example, we can add a new variable S that is influenced by R and influences C, but does not directly affect R. This creates an indirect causal path between R and C through S, which blocks the direct causal path between R and C and removes the influence of R on C.</p><p>Once the causal graph has been modified, the do operator can be used to manipulate the values of variables in the modified graph and estimate the causal effects of the interventions. For example, we can apply the do operator to set the value of R to a specific value, while holding all other variables constant, and observe the corresponding effect on C in the modified causal graph.</p><p>It is important to note that modifying the structure of the causal graph requires careful consideration of the underlying causal relationships and the potential impact of the modification on the validity of the causal inference. It is also important to conduct sensitivity analysis to assess the robustness of the results to different variations of the causal model.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div></body></html>