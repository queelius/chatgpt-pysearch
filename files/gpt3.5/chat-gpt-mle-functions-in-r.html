<!DOCTYPE html><html><head>  <meta charset="utf-8"/>  <title>Chat GPT: MLE Functions in R</title>  <meta name="generator" content="chatGPT Saving Bookmark"/><style>body {  background-color: rgb(32,33,35);  color: rgb(236,236,241);  font-size: 16px;  font-family: sans-serif;  line-height: 28px;  margin: 0;}body > .w-full {  padding: 30px;}/* prompt */body > .w-full:nth-child(2n+1) {  background: rgb(52,53,65);}/* response */body > .w-full:nth-child(2n+2) {  background: rgb(68,70,84);}a, a:visited {  color: #7792cd;}pre {  margin: 0 0 1em 0;  display: inline-block;  width: 100%;}pre code.hljs {  margin-bottom: 1em;  border-radius: 5px;}.whitespace-pre-wrap {  white-space: pre-wrap;}.flex-col {  max-width: 850px;  margin: 0px auto;}*, :after, :before {    border: 0 solid #d9d9e3;    box-sizing: border-box;}table {    border-collapse: collapse;    border-color: inherit;    text-indent: 0;}.markdown table {    --tw-border-spacing-x: 0px;    --tw-border-spacing-y: 0px;    border-collapse: separate;    border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y);    width: 100%}.markdown th {    background-color: rgba(236,236,241,.2);    border-bottom-width: 1px;    border-left-width: 1px;    border-top-width: 1px;    padding: .25rem .75rem}.markdown th:first-child {    border-top-left-radius: .375rem}.markdown th:last-child {    border-right-width: 1px;    border-top-right-radius: .375rem}.markdown td {    border-bottom-width: 1px;    border-left-width: 1px;    padding: .25rem .75rem}.markdown td:last-child {    border-right-width: 1px}.markdown tbody tr:last-child td:first-child {    border-bottom-left-radius: .375rem}.markdown tbody tr:last-child td:last-child {    border-bottom-right-radius: .375rem}/* chatGPT code color theme */code.hljs,code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#fff;-webkit-hyphens:none;hyphens:none;line-height:1.5;tab-size:4;text-align:left;white-space:pre;word-break:normal;word-spacing:normal}pre[class*=language-]{border-radius:.3em;overflow:auto}:not(pre)>code.hljs,:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.hljs-comment{color:hsla(0,0%,100%,.5)}.hljs-meta{color:hsla(0,0%,100%,.6)}.hljs-built_in,.hljs-class .hljs-title{color:#e9950c}.hljs-doctag,.hljs-formula,.hljs-keyword,.hljs-literal{color:#2e95d3}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#00a67d}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#df3079}.hljs-bullet,.hljs-link,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#f22c3d}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#a9aec1}.token.punctuation{color:#fefefe}.token.constant,.token.deleted,.token.property,.token.symbol,.token.tag{color:#ffa07a}.token.boolean,.token.number{color:#00e0e0}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#abe338}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url,.token.variable{color:#00e0e0}.token.atrule,.token.attr-value,.token.function{color:gold}.token.keyword{color:#00e0e0}.token.important,.token.regex{color:gold}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}@media screen and (-ms-high-contrast:active){code[class*=language-],pre[class*=language-]{background:window;color:windowText}:not(pre)>code[class*=language-],pre[class*=language-]{background:window}.token.important{background:highlight;color:window;font-weight:400}.token.atrule,.token.attr-value,.token.function,.token.keyword,.token.operator,.token.selector{font-weight:700}.token.attr-value,.token.comment,.token.doctype,.token.function,.token.keyword,.token.operator,.token.property,.token.string{color:highlight}.token.attr-value,.token.url{font-weight:400}}/* style of the code snippets */.rounded-md {    border-radius: 0.375rem;}.mb-4 {    margin-bottom: 1rem;}.p-4 {    padding: 1rem;}.py-2 {    padding-bottom: 0.5rem;    padding-top: 0.5rem;}.px-4 {    padding-left: 1rem;    padding-right: 1rem;}.text-xs {    font-size: .75rem;    line-height: 1rem;}.bg-black {    --tw-bg-opacity: 1;    background-color: rgba(0,0,0,var(--tw-bg-opacity));}.text-gray-200 {    --tw-text-opacity: 1;    color: rgba(217,217,227,var(--tw-text-opacity));}.bg-gray-800 {    --tw-bg-opacity: 1;    background-color: rgba(52,53,65,var(--tw-bg-opacity));}.rounded-t-md {    border-top-left-radius: 0.375rem;    border-top-right-radius: 0.375rem;}code.hljs, code[class*=language-], pre[class*=language-] {    word-wrap: normal;    background: none;    color: #fff;    -webkit-hyphens: none;    hyphens: none;    line-height: 1.5;    tab-size: 4;    text-align: left;    white-space: pre;    word-break: normal;    word-spacing: normal;}.prose :where(code):not(:where([class~=not-prose] *)) {    color: var(--tw-prose-code);    font-size: .875em;    font-weight: 600;}.prose :where(pre):not(:where([class~=not-prose] *)) {    background-color: transparent;    border-radius: 0.375rem;    color: currentColor;    font-size: .875em;    font-weight: 400;    line-height: 1.7142857;    margin: 0;    overflow-x: auto;    padding: 0;}.prose :where(pre code):not(:where([class~=not-prose] *)) {    background-color: transparent;    border-radius: 0;    border-width: 0;    color: inherit;    font-family: inherit;    font-size: inherit;    font-weight: inherit;    line-height: inherit;    padding: 0;}.\!whitespace-pre {    white-space: pre!important;}.overflow-y-auto {    overflow-y: auto;}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"/></head><body><div class="flex w-full items-center justify-center gap-1 border-b border-black/10 bg-gray-50 p-3 text-gray-500 dark:border-gray-900/50 dark:bg-gray-700 dark:text-gray-300">Model: Default (GPT-3.5)</div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Let's think about maximum likelihood estimation. What are some common methods or functions in R that I should implement as generic functions for this class of object?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>In R, common functions for maximum likelihood estimation (MLE) are:</p><ol><li><p><code>optim()</code>: a generic optimization function that can be used to maximize a variety of objective functions.</p></li><li><p><code>nlm()</code>: a function for optimizing non-linear objective functions.</p></li><li><p><code>nls()</code>: a function for fitting non-linear regression models via maximum likelihood estimation.</p></li><li><p><code>glm()</code>: a function for fitting generalized linear models, which can include maximum likelihood estimation of model parameters.</p></li><li><p><code>vglm()</code>: a function for fitting vector generalized linear models, which can also include maximum likelihood estimation.</p></li><li><p><code>fitdistr()</code>: a function for fitting univariate probability distributions to data using maximum likelihood estimation.</p></li></ol><p>It is also possible to implement custom MLE functions using lower-level R functions such as <code>optim()</code> or <code>nlm()</code>. However, for most common cases, these functions should be sufficient.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">I'm interested in `nlm` and `optim`.</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p><code>nlm()</code> (Nonlinear Optimization using Newton-Raphson Method) and <code>optim()</code> (General-Purpose Optimization) are two R functions for optimizing a given objective function. Here's a brief overview of each function:</p><ol><li><p><code>nlm()</code>: This function is used for optimizing non-linear objective functions. It uses the Newton-Raphson method to find the minimum or maximum of a function. The function provides starting values, which the algorithm then refines iteratively until convergence. <code>nlm()</code> is well-suited for optimizing smooth functions, but may not perform well for functions with multiple local minima or non-smooth functions.</p></li><li><p><code>optim()</code>: This is a more general-purpose optimization function that can be used to maximize or minimize a wide range of objective functions. The function can use a variety of optimization algorithms, including BFGS, L-BFGS-B, Nelder-Mead, and many others. The choice of algorithm can be specified by the user. <code>optim()</code> is more flexible than <code>nlm()</code>, but may be slower for certain types of functions.</p></li></ol><p>Both <code>nlm()</code> and <code>optim()</code> are widely used in R for maximum likelihood estimation and other optimization tasks, and can be useful for implementing generic functions for this class of object.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Do `nlm` and `optim` call compiled C or C++ code for speed reasons?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Yes, both <code>nlm</code> and <code>optim</code> in R call compiled C or C++ code for improved speed and performance. R is implemented in C and many of its built-in functions, including <code>nlm</code> and <code>optim</code>, are implemented in C or C++ for efficiency. The use of compiled code allows R to perform complex computations and optimization tasks much faster than it could if implemented entirely in R.</p><p>Additionally, the optimization algorithms used by <code>nlm</code> and <code>optim</code> are often complex and computationally intensive, and their implementation in C or C++ helps to improve the overall speed and performance of the optimization process. By calling compiled code, these functions can take advantage of lower-level optimizations and faster execution speeds, which are not available in pure R code.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Does numerically solving the MLE benefit from SGD?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>The numerical solution of the maximum likelihood estimation (MLE) problem may benefit from the use of stochastic gradient descent (SGD) under certain conditions.</p><p>SGD is a optimization algorithm that iteratively updates the parameters of a model in the direction of the negative gradient of the objective function. SGD has been shown to be an effective optimization method for large-scale problems where the data set is too big to be processed all at once, or where the objective function is non-convex.</p><p>In the context of MLE, SGD can be used to optimize the likelihood function by updating the parameters in the direction of the negative gradient of the log-likelihood function. This can be particularly useful for problems with large data sets, where traditional optimization methods, such as the Newton-Raphson method used by <code>nlm</code>, may be too slow or computationally expensive.</p><p>However, it's important to note that the choice of optimization algorithm depends on the specific problem being solved, and SGD may not always be the best choice. For example, traditional optimization methods may be more appropriate for problems with small data sets, or where the objective function is well-behaved and has a unique global optimum. In such cases, SGD may be slow to converge or may get stuck in a local optimum.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">What about a stochastic version of Newton-Raphson?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>A stochastic version of the Newton-Raphson method can be used in the context of maximum likelihood estimation (MLE). This approach combines the ideas of stochastic gradient descent (SGD) with the Newton-Raphson method to create a hybrid optimization algorithm that can be used to solve MLE problems.</p><p>In this approach, instead of computing the gradient of the log-likelihood function for the entire data set, a small random subset of the data is selected to compute the gradient. This provides a stochastic approximation of the gradient, which is then used to update the parameters in the direction of the negative gradient.</p><p>The stochastic Newton-Raphson method can be seen as a trade-off between SGD and the traditional Newton-Raphson method. Like SGD, it can handle large-scale problems and deal with non-convex objective functions, but it retains some of the strengths of the Newton-Raphson method, such as its ability to converge rapidly to the optimum.</p><p>It's important to note that the choice of optimization algorithm depends on the specific problem being solved, and the stochastic Newton-Raphson method may not always be the best choice. The trade-off between accuracy and computation time can be complex, and the appropriate choice of algorithm will depend on the specific characteristics of the problem and the desired results.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">What are some approaches for getting out of local optimums?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Here are some approaches that can be used to overcome getting stuck in local optima in maximum likelihood estimation (MLE) problems:</p><ol><li><p>Multi-start optimization: In this approach, the optimization process is run multiple times, each time starting from a different initial parameter value. By repeating the optimization process, it is possible to escape from local optima and find the global optimum.</p></li><li><p>Simulated annealing: This is a meta-optimization method that can be used to escape from local optima. Simulated annealing is a probabilistic method that randomly perturbs the parameters in order to escape from a local optimum. The method is inspired by annealing in metallurgy, where a material is heated and cooled to remove impurities.</p></li><li><p>Genetic algorithms: This is a class of optimization algorithms inspired by the process of natural selection. In genetic algorithms, a population of candidate solutions is evolved over multiple generations through a process of selection, crossover, and mutation. The algorithm can be used to escape from local optima and find the global optimum.</p></li><li><p>Gradient-based methods with restarts: This is a class of optimization algorithms that combine gradient-based optimization with restarts. The algorithm runs multiple gradient-based optimization algorithms, each time starting from a different initial parameter value. By repeating the optimization process, it is possible to escape from local optima and find the global optimum.</p></li><li><p>Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the objective function. By adding a regularization term, it is possible to bias the optimization towards a simpler solution, which can help to escape from local optima.</p></li></ol><p>These are some of the common approaches that can be used to escape from local optima in MLE problems. The choice of approach will depend on the specific problem being solved and the desired results.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Tell me more about 2.</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Simulated annealing is a meta-optimization method that can be used to escape from local optima in maximum likelihood estimation (MLE) problems. The method is a probabilistic approach that can be used to find the global optimum of a non-convex objective function.</p><p>The idea behind simulated annealing is inspired by the process of annealing in metallurgy, where a material is heated and cooled to remove impurities. Similarly, simulated annealing uses a temperature parameter to control the randomness of the optimization process. At high temperatures, the algorithm is free to make large random moves in the parameter space, which can help it to escape from local optima. As the temperature decreases, the algorithm becomes more constrained, and the randomness of the moves decreases, allowing it to converge to the optimum.</p><p>The algorithm works by iteratively updating the parameters by making small random perturbations to the current parameter values. The perturbations are accepted or rejected based on a probabilistic acceptance criterion that depends on the difference between the objective function value at the current and proposed parameter values, and the temperature.</p><p>At each iteration, the temperature is decreased according to a cooling schedule, which determines the rate at which the randomness of the algorithm decreases. The cooling schedule can be adjusted to control the trade-off between convergence speed and the ability to escape from local optima.</p><p>Simulated annealing is a useful method for MLE problems where the objective function is non-convex and has multiple local optima. The method can be used to escape from local optima and find the global optimum, but it can also be slow to converge and may not always find the global optimum. The choice of optimization algorithm depends on the specific problem being solved, and the desired results.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Show me a simple example in R?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Here's a simple example of how to use simulated annealing in R to find the global optimum of a two-dimensional non-convex objective function:</p><pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>r</span></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-r"><span class="hljs-comment"># Define the objective function</span>
objective_function <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span><span class="hljs-operator">-</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-operator">^</span><span class="hljs-number">2</span> <span class="hljs-operator">+</span> x<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">]</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># Simple parabolic function</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the acceptance probability function</span>
acceptance_prob <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>dE<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span><span class="hljs-built_in">exp</span><span class="hljs-punctuation">(</span><span class="hljs-operator">-</span>dE<span class="hljs-operator">/</span><span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># Boltzmann distribution</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the simulated annealing algorithm</span>
simulated_annealing <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  x <span class="hljs-operator">&lt;-</span> start
  <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> T_start
  <span class="hljs-keyword">for</span> <span class="hljs-punctuation">(</span>i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span><span class="hljs-operator">:</span>max_iter<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-comment"># Generate random perturbation</span>
    x_new <span class="hljs-operator">&lt;-</span> x <span class="hljs-operator">+</span> rnorm<span class="hljs-punctuation">(</span><span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Calculate energy difference</span>
    delta_E <span class="hljs-operator">&lt;-</span> obj_func<span class="hljs-punctuation">(</span>x_new<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Accept or reject perturbation based on acceptance probability</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span>runif<span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">&lt;</span> acceptance_prob<span class="hljs-punctuation">(</span>delta_E<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      x <span class="hljs-operator">&lt;-</span> x_new
    <span class="hljs-punctuation">}</span>
    <span class="hljs-comment"># Decrease temperature according to cooling schedule</span>
    <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">T</span> <span class="hljs-operator">*</span> alpha
    <span class="hljs-comment"># Check if temperature has reached the end temperature</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;</span> T_end<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      <span class="hljs-keyword">break</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">}</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define starting parameters and run simulated annealing</span>
start <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
T_start <span class="hljs-operator">&lt;-</span> 10
T_end <span class="hljs-operator">&lt;-</span> 0.001
alpha <span class="hljs-operator">&lt;-</span> 0.98
max_iter <span class="hljs-operator">&lt;-</span> 10000
optimal_params <span class="hljs-operator">&lt;-</span> simulated_annealing<span class="hljs-punctuation">(</span>objective_function<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">)</span>

<span class="hljs-comment"># Print the optimal parameters</span>
print<span class="hljs-punctuation">(</span>optimal_params<span class="hljs-punctuation">)</span>
</code></div></div></pre><p>This simple example defines a parabolic objective function and an acceptance probability function based on the Boltzmann distribution. The simulated annealing algorithm iteratively updates the parameters by making small random perturbations, and accepting or rejecting the perturbations based on the acceptance probability. The temperature is gradually decreased according to a cooling schedule, and the algorithm continues until the temperature reaches the end temperature or the maximum number of iterations is reached. The optimal parameters are returned by the algorithm and can be used to minimize the objective function.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Can we combine simulated annealing with gradient-based methods?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Yes, simulated annealing can be combined with gradient-based methods to form hybrid optimization algorithms. Hybrid algorithms combine the strengths of both methods to find the global optimum of a non-convex objective function.</p><p>One popular hybrid approach is the simulated annealing with gradient descent (SA-GD) method. This method uses simulated annealing to escape from local optima and gradient descent to converge to the global optimum. The algorithm alternates between making random perturbations to the parameters (simulated annealing step) and updating the parameters based on the gradient of the objective function (gradient descent step). The simulated annealing step can be used to escape from local optima, while the gradient descent step can be used to rapidly converge to the optimum.</p><p>The SA-GD method can be useful for problems where the objective function is non-convex and has multiple local optima, but also has a well-defined gradient that can be used for optimization. The method combines the ability of simulated annealing to escape from local optima with the speed of convergence of gradient-based methods.</p><p>In R, the optimization library provides several functions for performing gradient-based optimization, such as <code>optim</code> and <code>optimize</code>, and these functions can be used to implement the SA-GD method in combination with the simulated annealing algorithm described in the previous example.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Show me an example in R?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Here's an example of how to implement the simulated annealing with gradient descent (SA-GD) method in R:</p><pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>r</span></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-r"><span class="hljs-comment"># Define the objective function</span>
objective_function <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span><span class="hljs-operator">-</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-operator">^</span><span class="hljs-number">2</span> <span class="hljs-operator">+</span> x<span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">]</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># Simple parabolic function</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the gradient of the objective function</span>
grad_obj_func <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span><span class="hljs-operator">-</span><span class="hljs-number">2</span> <span class="hljs-operator">*</span> x<span class="hljs-punctuation">)</span> <span class="hljs-comment"># Gradient of the parabolic function</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the acceptance probability function</span>
acceptance_prob <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>dE<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span><span class="hljs-built_in">exp</span><span class="hljs-punctuation">(</span><span class="hljs-operator">-</span>dE<span class="hljs-operator">/</span><span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># Boltzmann distribution</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the simulated annealing with gradient descent algorithm</span>
SA_GD <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">,</span> grad_obj_func<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">,</span> step_size<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  x <span class="hljs-operator">&lt;-</span> start
  <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> T_start
  <span class="hljs-keyword">for</span> <span class="hljs-punctuation">(</span>i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span><span class="hljs-operator">:</span>max_iter<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-comment"># Make a random perturbation with simulated annealing</span>
    x_new <span class="hljs-operator">&lt;-</span> x <span class="hljs-operator">+</span> rnorm<span class="hljs-punctuation">(</span><span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Calculate energy difference</span>
    delta_E <span class="hljs-operator">&lt;-</span> obj_func<span class="hljs-punctuation">(</span>x_new<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Accept or reject perturbation based on acceptance probability</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span>runif<span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">&lt;</span> acceptance_prob<span class="hljs-punctuation">(</span>delta_E<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      x <span class="hljs-operator">&lt;-</span> x_new
    <span class="hljs-punctuation">}</span>
    <span class="hljs-comment"># Update parameters with gradient descent</span>
    x <span class="hljs-operator">&lt;-</span> x <span class="hljs-operator">-</span> step_size <span class="hljs-operator">*</span> grad_obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Decrease temperature according to cooling schedule</span>
    <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">T</span> <span class="hljs-operator">*</span> alpha
    <span class="hljs-comment"># Check if temperature has reached the end temperature</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;</span> T_end<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      <span class="hljs-keyword">break</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">}</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define starting parameters and run SA-GD</span>
start <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
T_start <span class="hljs-operator">&lt;-</span> 10
T_end <span class="hljs-operator">&lt;-</span> 0.001
alpha <span class="hljs-operator">&lt;-</span> 0.98
max_iter <span class="hljs-operator">&lt;-</span> 10000
step_size <span class="hljs-operator">&lt;-</span> 0.1
optimal_params <span class="hljs-operator">&lt;-</span> SA_GD<span class="hljs-punctuation">(</span>objective_function<span class="hljs-punctuation">,</span> grad_obj_func<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">,</span> step_size<span class="hljs-punctuation">)</span>

<span class="hljs-comment"># Print the optimal parameters</span>
print<span class="hljs-punctuation">(</span>optimal_params<span class="hljs-punctuation">)</span>
</code></div></div></pre><p>In this example, the objective function and its gradient are defined as the parabolic function and its gradient, respectively. The SA-GD algorithm combines simulated annealing with gradient descent by making random perturbations to the parameters (simulated annealing step) and updating the parameters based on the gradient of the objective function (gradient descent step). The optimal parameters are returned by the algorithm and can be used to minimize the objective function.</p></div></div></div><div class="flex justify-between lg:block"></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 dark:bg-gray-800"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">Can we incorporate line search methods into the above as well?</div></div></div></div></div><div class="group w-full text-gray-800 dark:text-gray-100 border-b border-black/10 dark:border-gray-900/50 bg-gray-50 dark:bg-[#444654]"><div class="text-base gap-4 md:gap-6 md:max-w-2xl lg:max-w-xl xl:max-w-3xl p-4 md:py-6 flex lg:px-0 m-auto"><div class="relative flex w-[calc(100%-50px)] flex-col gap-1 md:gap-3 lg:w-[calc(100%-115px)]"><div class="flex flex-grow flex-col gap-3"><div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap"><div class="markdown prose w-full break-words dark:prose-invert dark"><p>Yes, line search methods can be incorporated into the simulated annealing with gradient descent (SA-GD) method. Line search methods are used to determine the step size in gradient descent by finding the step size that results in the largest decrease in the objective function. By incorporating line search methods into SA-GD, the algorithm can dynamically adjust the step size in each iteration to ensure faster convergence.</p><p>There are several line search methods that can be used, such as backtracking line search, Armijo rule, and Wolfe conditions. These methods use different criteria to determine the step size, such as the decrease in the objective function or the slope of the objective function at the current step.</p><p>To incorporate line search methods into the SA-GD algorithm, simply replace the fixed step size in each iteration with a call to a line search method that determines the optimal step size based on the gradient of the objective function. The line search method should be called before updating the parameters with gradient descent.</p><p>Here's an example of how to incorporate backtracking line search into the SA-GD algorithm in R:</p><pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>r</span></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-r"><span class="hljs-comment"># Define the backtracking line search algorithm</span>
backtracking_line_search <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">,</span> x<span class="hljs-punctuation">,</span> d<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> beta<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  t <span class="hljs-operator">&lt;-</span> 1
  <span class="hljs-keyword">while</span> <span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">(</span>x <span class="hljs-operator">+</span> t <span class="hljs-operator">*</span> d<span class="hljs-punctuation">)</span> <span class="hljs-operator">&gt;</span> obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">+</span> alpha <span class="hljs-operator">*</span> t <span class="hljs-operator">*</span> grad_obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">%*%</span> d<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    t <span class="hljs-operator">&lt;-</span> t <span class="hljs-operator">*</span> beta
  <span class="hljs-punctuation">}</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span>t<span class="hljs-punctuation">)</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define the simulated annealing with gradient descent and line search algorithm</span>
SA_GD_LS <span class="hljs-operator">&lt;-</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">,</span> grad_obj_func<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">,</span> alpha_ls<span class="hljs-punctuation">,</span> beta_ls<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
  x <span class="hljs-operator">&lt;-</span> start
  <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> T_start
  <span class="hljs-keyword">for</span> <span class="hljs-punctuation">(</span>i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span><span class="hljs-operator">:</span>max_iter<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-comment"># Make a random perturbation with simulated annealing</span>
    x_new <span class="hljs-operator">&lt;-</span> x <span class="hljs-operator">+</span> rnorm<span class="hljs-punctuation">(</span><span class="hljs-built_in">length</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Calculate energy difference</span>
    delta_E <span class="hljs-operator">&lt;-</span> obj_func<span class="hljs-punctuation">(</span>x_new<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Accept or reject perturbation based on acceptance probability</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span>runif<span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">&lt;</span> acceptance_prob<span class="hljs-punctuation">(</span>delta_E<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      x <span class="hljs-operator">&lt;-</span> x_new
    <span class="hljs-punctuation">}</span>
    <span class="hljs-comment"># Determine step size with backtracking line search</span>
    d <span class="hljs-operator">&lt;-</span> <span class="hljs-operator">-</span>grad_obj_func<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
    step_size <span class="hljs-operator">&lt;-</span> backtracking_line_search<span class="hljs-punctuation">(</span>obj_func<span class="hljs-punctuation">,</span> x<span class="hljs-punctuation">,</span> d<span class="hljs-punctuation">,</span> alpha_ls<span class="hljs-punctuation">,</span> beta_ls<span class="hljs-punctuation">)</span>
    <span class="hljs-comment"># Update parameters with gradient descent</span>
    x <span class="hljs-operator">&lt;-</span> x <span class="hljs-operator">+</span> step_size <span class="hljs-operator">*</span> d
    <span class="hljs-comment"># Decrease temperature according to cooling schedule</span>
    <span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">T</span> <span class="hljs-operator">*</span> alpha
    <span class="hljs-comment"># Check if temperature has reached the end temperature</span>
    <span class="hljs-keyword">if</span> <span class="hljs-punctuation">(</span><span class="hljs-built_in">T</span> <span class="hljs-operator">&lt;</span> T_end<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
      <span class="hljs-keyword">break</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">}</span>
  <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span>
<span class="hljs-punctuation">}</span>

<span class="hljs-comment"># Define starting parameters and run SA-GD with line search</span>
start <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
T_start <span class="hljs-operator">&lt;-</span> 10
T_end <span class="hljs-operator">&lt;-</span> 0.001
alpha <span class="hljs-operator">&lt;-</span> 0.98
max_iter <span class="hljs-operator">&lt;-</span> 10000
alpha_ls <span class="hljs-operator">&lt;-</span> 0.5
beta_ls <span class="hljs-operator">&lt;-</span> 0.5
optimal_params <span class="hljs-operator">&lt;-</span> SA_GD_LS<span class="hljs-punctuation">(</span>objective_function<span class="hljs-punctuation">,</span> grad_obj_func<span class="hljs-punctuation">,</span> start<span class="hljs-punctuation">,</span> T_start<span class="hljs-punctuation">,</span> T_end<span class="hljs-punctuation">,</span> alpha<span class="hljs-punctuation">,</span> max_iter<span class="hljs-punctuation">,</span> alpha_ls<span class="hljs-punctuation">,</span> beta_ls<span class="hljs-punctuation">)</span>

<span class="hljs-comment"># Print the</span>
</code></div></div></pre></div></div></div><div class="flex justify-between lg:block"></div></div></div></div></body></html>